# -*- coding: utf-8 -*-
"""11/20혼공데.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lrfXsaIyYW4VmfzL-Ud9-G7f1tTIcxcv

3-1. 불필요한 데이터 삭제
"""

import gdown
gdown.download('https://bit.ly/3RhoNho', 'ns_202104.csv', quiet=False)

import pandas as pd
ns_df = pd.read_csv('ns_202104.csv', low_memory=False)
ns_df.head()

ns_book=ns_df.loc[:, '번호':'등록일자'] #마지막열도 출력(등록일자까지)
ns_book.head()

selected_columns = ns_df.columns != '부가기호'
ns_book = ns_df.loc[:, selected_columns]
ns_book.head()

ns_book = ns_df.drop('Unnamed: 13', axis=1)
ns_book.head()

ns_book.drop('주제분류번호', axis=1, inplace=True)
ns_book.head()

ns_book = ns_df.dropna(axis=1, how='all')
ns_book.head()

ns_book2 = ns_book.drop([0,1])
ns_book2.head()

ns_book2 = ns_book[0:2]
ns_book2.head()

selected_rows = ns_df['출판사'] == '한빛미디어'
ns_book2 = ns_book[selected_rows]
ns_book2.head()

ns_book2 = ns_book[ns_book['대출건수']>1000]
ns_book2.head()

sum(ns_book.duplicated())

sum(ns_book.duplicated(subset=['도서명','저자','ISBN']))

dup_rows= ns_book.duplicated(subset=['도서명','저자','ISBN'],keep=False)
ns_book3 = ns_book[dup_rows]
ns_book3.head()

count_df=ns_book[['도서명', '저자', 'ISBN', '권', '대출건수']]
group_df = count_df.groupby(by=['도서명','저자','ISBN','권'],dropna=False)
loan_cunt=group_df.sum()
loan_count = count_df.groupby(by=['도서명', '저자', 'ISBN', '권'], dropna=False).sum()
loan_count.head()



# filename을 입력으로 받는 data_cleaning이라는 함수를 정의합니다.
def data_cleaning(filename):

  # 주어진 파일명으로 CSV 파일을 pandas DataFrame인 ns_df로 읽어옵니다.
  ns_df = pd.read_csv(filename, low_memory=False)

  # ns_df에서 모든 값이 NaN인 열을 제거하고 결과를 ns_book에 저장합니다.
  ns_book = ns_df.dropna(axis=1, how='all')

  # '도서명', '저자', 'ISBN', '권' 열을 선택하여 새로운 DataFrame(count_df)을 만듭니다.
  count_df = ns_book[['도서명', '저자', 'ISBN', '권', '대출건수']]

  # count_df를 '도서명', '저자', 'ISBN', '권'을 기준으로 그룹화하고 대출건수를 합산합니다.
  loan_count = count_df.groupby(by=['도서명', '저자', 'ISBN', '권'], dropna=False).sum()

  # ns_book에서 중복된 행을 찾습니다.
  dup_rows = ns_book.duplicated(subset=['도서명', '저자', 'ISBN', '권'])

  # 중복되지 않은 행을 찾기 위해 unique_rows를 만듭니다.
  unique_rows = ~dup_rows

  # 중복이 제거된 ns_book을 복사하여 ns_book3에 저장합니다.
  ns_book3 = ns_book[unique_rows].copy()

  # '도서명', '저자', 'ISBN', '권'을 인덱스로 설정합니다.
  ns_book3.set_index(['도서명', '저자', 'ISBN', '권'], inplace=True)

  # ns_book3을 loan_count로 업데이트합니다.
  ns_book3.update(loan_count)

  # 인덱스를 재설정하고 ns_book4에 저장합니다.
  ns_book4 = ns_book3.reset_index()

  # 원본 열 순서를 유지하면서 ns_book4에서 ns_book 열을 선택합니다.
  ns_book4 = ns_book4[ns_book.columns]

  # 최종적으로 정리된 ns_book4를 반환합니다.
  return ns_book4

"""3-2장 잘못된 데이터 수정"""

import gdown
gdown.download('https://bit.ly/3GisL6J', 'ns_book4.csv', quiet=False)
import pandas as pd
ns_book4 = pd.read_csv('ns_book4.csv', low_memory=False)
ns_book4.head()

ns_book4.isna().sum()

ns_book4.loc[0, '도서권수'] = None
ns_book4['도서권수'].isna().sum()

ns_book4.loc[0, '부가기호'] = None
ns_book4.head(2)

import numpy as np
ns_book4.loc[0, '부가기호'] = np.nan
ns_book4.head(2)

ns_book4.fillna('없음').isna().sum()

ns_book4.replace(np.nan, '없음').isna().sum()

ns_book4.replace([np.nan, '2021'],['없음','21']).head(2)

ns_book4.replace({np.nan : '없음', '2021':'21'}).head()

ns_book4.replace({'부가기호':np.nan}, '없음').head(2)

ns_book4.replace({'부가기호':{np.nan: '없음'}, '발행년도':{'2021':'21'}}).head(2)

"""4-1 통계로 요약하기"""

# gdown 라이브러리를 가져옵니다. 이는 Google Drive에서 파일을 다운로드하기 위한 Python 라이브러리입니다.
import gdown

# 지정된 URL ('https://bit.ly/3736JW1')에서 파일을 다운로드하고 'ns_book6.csv'라는 이름으로 저장합니다.
# 'quiet' 매개변수는 False로 설정되어 있어 다운로드 진행 상황이 표시됩니다.
gdown.download('https://bit.ly/3736JW1', 'ns_book6.csv', quiet=False)

import pandas as pd
ns_book6 = pd.read_csv('ns_book6.csv', low_memory=False)
ns_book6.head()

ns_book6.describe()

"""<메서드>
-count: 개수
-mean :평균
-std: 표준편차
-min: 최솟값
-50% : 중앙값
-25%와 75% : 순서대로 늘어 놓았을 때 25%지점과 75%지점에 놓인값
-max : 최댓값

"""

ns_book7=ns_book6[ns_book6['도서권수']>0]
ns_book7.describ(include='object')



