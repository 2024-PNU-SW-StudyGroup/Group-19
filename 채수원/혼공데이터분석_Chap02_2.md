# 웹 스크래핑 사용하기

## 필요한 정보를 제공하는 API가 없는 경우
특정 사이트의 URL을 request.get()으로 가져와서 데이터를 추출할 수 있음.
-> 이는 웹 크롤링, 웹 스크래핑 이라고 한다.
## YES24에서 검색 결과 페이지 가져오기
1. Pandas의 'loc'메서드 활용하여 df의 행과 열 가져오기 ex) -> df.loc[[0, 1],['bookname', 'authors']]
2. iloc(IndexLocation) 메서드는? loc메서드는 index의 name을 parameter로 사용하지만 iloc은 index number를 사용함. 또한 해당 메서드들은 ':' 슬라이스 연산자 사용가능. but 마지막 항목 포함.
3. Chrome(크로미움)의 개발자 도구를 활용하여 우리가 가져와야 할 데이터가 있는 부분을 특정.

### HTML에서 데이터 추출하기 : 뷰티풀수프 or Scrapy
1. 크롬 개발자 도구(F12)를 활성화하여 request.get() 메서드로 불러온 HTML data중에 우리가 필요한 data의 위치를 특정합니다.
2. from bs4 import BeautifulSoup

parser -  입력 데이터를 받아 데이터 구조로 만들어줌

parsing - 파서를 사용하는 과정

BeautifulSoup는 기본 내장 lxml패키지를 활용하여 파싱하는 것이 기본 세팅. lxml패키지는 html5문법을 매우 엄격히 검사하기 때문에 개발자들은 보통 html.parser 를 사용함.

3. 정보가 있는 객체의 위치 찾기.  -  soup.find() 메서드 EX) - link = soup.find('a', attrs={'class':'gd_name'})
태그, 속성, 이름

보통 class로 데이터를 불러오는 것이 좋다. div나 table같은 태그로 불러오게 되면 단일속성이라고 단언할 수 없기 때문에 100% 내가 원하는 데이터가 늘 가져와진다고 장담할 수 없음.

find_all() 메서드를 활용하면 특정 <>태그를 모두 찾아서 리스트로 반환해줌.

get_text() 메서드를 활용하면 태그 안의 텍스트를 추출할 수 있다.

apply() 메서드를 활용하여 df의 특정 열, 행 전체에 특정 메서드를 적용시킬 수 있다.

merge() 메서드를 활용하면 df에 시리즈를 합칠 수 있다. (추출한 도서 페이지 수를 df에 추가)

## 웹스크래핑 주의할 점
1. 웹 스크래핑은 웹 서버에 굉장히 많은 요청을 날려 서버부하를 야기함
2. 특정 웹 사이트(서버)에서 밴을 할 수 있음
3. 해당 사이트가 웹 크롤링을 허용하는지 미리 확인해야 한다.
4. 모든 페이지는 robots.txt를 가지고 있는데 이를 열람하면 해당 웹 페이지의 제한사항을 확인할 수 있다.